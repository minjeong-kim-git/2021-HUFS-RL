{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_ppo.ipynb의 사본",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minjeong-kim-git/HUFS-RL-2022/blob/main/Day%2007/simple_ppo_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PPO 폴더 만듦\n",
        "!mkdir PPO"
      ],
      "metadata": {
        "id": "6rLu467vAbkx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "class PPOMemory:\n",
        "    def __init__(self, batch_size):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.vals = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def generate_batches(self):\n",
        "        n_states = len(self.states)\n",
        "        batch_start = np.arange(0, n_states, self.batch_size)\n",
        "        indices = np.arange(n_states, dtype=np.int64)\n",
        "        np.random.shuffle(indices)\n",
        "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
        "\n",
        "        return np.array(self.states),\\\n",
        "                np.array(self.actions),\\\n",
        "                np.array(self.probs),\\\n",
        "                np.array(self.vals),\\\n",
        "                np.array(self.rewards),\\\n",
        "                np.array(self.dones),\\\n",
        "                batches\n",
        "\n",
        "    def store_memory(self, state, action, probs, vals, reward, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.probs.append(probs)\n",
        "        self.vals.append(vals)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "\n",
        "    def clear_memory(self):\n",
        "        self.states = []\n",
        "        self.probs = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.dones = []\n",
        "        self.vals = []\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_actions, input_dims, alpha,\n",
        "            fc1_dims=256, fc2_dims=256, chkpt_dir='PPO'):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
        "        self.actor = nn.Sequential(\n",
        "                nn.Linear(*input_dims, fc1_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc1_dims, fc2_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc2_dims, n_actions),\n",
        "                nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        dist = self.actor(state)\n",
        "        dist = Categorical(dist)\n",
        "        \n",
        "        return dist\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
        "            chkpt_dir='PPO'):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "\n",
        "        self.checkpoint_file = os.path.join(chkpt_dir, 'critic_torch_ppo')\n",
        "        self.critic = nn.Sequential(\n",
        "                nn.Linear(*input_dims, fc1_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc1_dims, fc2_dims),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(fc2_dims, 1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        value = self.critic(state)\n",
        "\n",
        "        return value\n",
        "\n",
        "    def save_checkpoint(self):\n",
        "        T.save(self.state_dict(), self.checkpoint_file)\n",
        "\n",
        "    def load_checkpoint(self):\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
        "            policy_clip=0.2, batch_size=64, n_epochs=10):\n",
        "        self.gamma = gamma\n",
        "        self.policy_clip = policy_clip\n",
        "        self.n_epochs = n_epochs\n",
        "        self.gae_lambda = gae_lambda\n",
        "\n",
        "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
        "        self.critic = CriticNetwork(input_dims, alpha)\n",
        "        self.memory = PPOMemory(batch_size)\n",
        "       \n",
        "    def remember(self, state, action, probs, vals, reward, done):\n",
        "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
        "\n",
        "    def save_models(self):\n",
        "        print('... saving models ...')\n",
        "        self.actor.save_checkpoint()\n",
        "        self.critic.save_checkpoint()\n",
        "\n",
        "    def load_models(self):\n",
        "        print('... loading models ...')\n",
        "        self.actor.load_checkpoint()\n",
        "        self.critic.load_checkpoint()\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
        "\n",
        "        dist = self.actor(state)\n",
        "        value = self.critic(state)\n",
        "        action = dist.sample()\n",
        "\n",
        "        probs = T.squeeze(dist.log_prob(action)).item()\n",
        "        action = T.squeeze(action).item()\n",
        "        value = T.squeeze(value).item()\n",
        "\n",
        "        return action, probs, value\n",
        "\n",
        "    def learn(self):\n",
        "        for _ in range(self.n_epochs):\n",
        "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
        "            reward_arr, dones_arr, batches = \\\n",
        "                    self.memory.generate_batches()\n",
        "\n",
        "            values = vals_arr\n",
        "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
        "\n",
        "            for t in range(len(reward_arr)-1):\n",
        "                discount = 1\n",
        "                a_t = 0\n",
        "                for k in range(t, len(reward_arr)-1):\n",
        "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
        "                            (1-int(dones_arr[k])) - values[k])\n",
        "                    discount *= self.gamma*self.gae_lambda\n",
        "                advantage[t] = a_t\n",
        "            advantage = T.tensor(advantage).to(self.actor.device)\n",
        "\n",
        "            values = T.tensor(values).to(self.actor.device)\n",
        "            for batch in batches:\n",
        "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
        "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
        "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
        "\n",
        "                dist = self.actor(states)\n",
        "                critic_value = self.critic(states)\n",
        "\n",
        "                critic_value = T.squeeze(critic_value)\n",
        "\n",
        "                new_probs = dist.log_prob(actions)\n",
        "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
        "                #prob_ratio = (new_probs - old_probs).exp()\n",
        "                weighted_probs = advantage[batch] * prob_ratio\n",
        "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
        "                        1+self.policy_clip)*advantage[batch]\n",
        "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
        "\n",
        "                returns = advantage[batch] + values[batch]\n",
        "                critic_loss = (returns-critic_value)**2\n",
        "                critic_loss = critic_loss.mean()\n",
        "\n",
        "                total_loss = actor_loss + 0.5*critic_loss\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                self.critic.optimizer.zero_grad()\n",
        "                total_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "                self.critic.optimizer.step()\n",
        "\n",
        "        self.memory.clear_memory()               \n",
        "\n"
      ],
      "metadata": {
        "id": "E16zWxaLAk6I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curve(x, scores, figure_file):\n",
        "    running_avg = np.zeros(len(scores))\n",
        "    for i in range(len(running_avg)):\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
        "    plt.plot(x, running_avg)\n",
        "    plt.title('Running average of previous 100 scores')\n",
        "    plt.savefig(figure_file)"
      ],
      "metadata": {
        "id": "FjiGDZ7sAtWy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "N = 20\n",
        "batch_size = 5\n",
        "n_epochs = 4\n",
        "alpha = 0.0003\n",
        "agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, \n",
        "                alpha=alpha, n_epochs=n_epochs, \n",
        "                input_dims=env.observation_space.shape)\n",
        "n_games = 300\n",
        "\n",
        "figure_file = 'PPO/cartpole.png'\n",
        "\n",
        "best_score = env.reward_range[0]\n",
        "score_history = []\n",
        "\n",
        "learn_iters = 0\n",
        "avg_score = 0\n",
        "n_steps = 0\n",
        "\n",
        "for i in range(n_games):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        action, prob, val = agent.choose_action(observation)\n",
        "        observation_, reward, done, info = env.step(action)\n",
        "        n_steps += 1\n",
        "        score += reward\n",
        "        agent.remember(observation, action, prob, val, reward, done)\n",
        "        if n_steps % N == 0:\n",
        "            agent.learn()\n",
        "            learn_iters += 1\n",
        "        observation = observation_\n",
        "    score_history.append(score)\n",
        "    avg_score = np.mean(score_history[-100:])\n",
        "\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "        agent.save_models()\n",
        "\n",
        "    print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
        "            'time_steps', n_steps, 'learning_steps', learn_iters)\n",
        "x = [i+1 for i in range(len(score_history))]\n",
        "plot_learning_curve(x, score_history, figure_file)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ptfy20jLA1ED",
        "outputId": "19bb9327-43e6-4792-9759-378635416f0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... saving models ...\n",
            "episode 0 score 36.0 avg score 36.0 time_steps 36 learning_steps 1\n",
            "episode 1 score 17.0 avg score 26.5 time_steps 53 learning_steps 2\n",
            "episode 2 score 10.0 avg score 21.0 time_steps 63 learning_steps 3\n",
            "episode 3 score 15.0 avg score 19.5 time_steps 78 learning_steps 3\n",
            "episode 4 score 18.0 avg score 19.2 time_steps 96 learning_steps 4\n",
            "episode 5 score 32.0 avg score 21.3 time_steps 128 learning_steps 6\n",
            "episode 6 score 14.0 avg score 20.3 time_steps 142 learning_steps 7\n",
            "episode 7 score 19.0 avg score 20.1 time_steps 161 learning_steps 8\n",
            "episode 8 score 15.0 avg score 19.6 time_steps 176 learning_steps 8\n",
            "episode 9 score 13.0 avg score 18.9 time_steps 189 learning_steps 9\n",
            "episode 10 score 23.0 avg score 19.3 time_steps 212 learning_steps 10\n",
            "episode 11 score 20.0 avg score 19.3 time_steps 232 learning_steps 11\n",
            "episode 12 score 15.0 avg score 19.0 time_steps 247 learning_steps 12\n",
            "episode 13 score 26.0 avg score 19.5 time_steps 273 learning_steps 13\n",
            "episode 14 score 15.0 avg score 19.2 time_steps 288 learning_steps 14\n",
            "episode 15 score 27.0 avg score 19.7 time_steps 315 learning_steps 15\n",
            "episode 16 score 20.0 avg score 19.7 time_steps 335 learning_steps 16\n",
            "episode 17 score 14.0 avg score 19.4 time_steps 349 learning_steps 17\n",
            "episode 18 score 18.0 avg score 19.3 time_steps 367 learning_steps 18\n",
            "episode 19 score 13.0 avg score 19.0 time_steps 380 learning_steps 19\n",
            "episode 20 score 9.0 avg score 18.5 time_steps 389 learning_steps 19\n",
            "episode 21 score 9.0 avg score 18.1 time_steps 398 learning_steps 19\n",
            "episode 22 score 9.0 avg score 17.7 time_steps 407 learning_steps 20\n",
            "episode 23 score 16.0 avg score 17.6 time_steps 423 learning_steps 21\n",
            "episode 24 score 20.0 avg score 17.7 time_steps 443 learning_steps 22\n",
            "episode 25 score 28.0 avg score 18.1 time_steps 471 learning_steps 23\n",
            "episode 26 score 15.0 avg score 18.0 time_steps 486 learning_steps 24\n",
            "episode 27 score 13.0 avg score 17.8 time_steps 499 learning_steps 24\n",
            "episode 28 score 15.0 avg score 17.7 time_steps 514 learning_steps 25\n",
            "episode 29 score 50.0 avg score 18.8 time_steps 564 learning_steps 28\n",
            "episode 30 score 37.0 avg score 19.4 time_steps 601 learning_steps 30\n",
            "episode 31 score 25.0 avg score 19.6 time_steps 626 learning_steps 31\n",
            "episode 32 score 19.0 avg score 19.5 time_steps 645 learning_steps 32\n",
            "episode 33 score 12.0 avg score 19.3 time_steps 657 learning_steps 32\n",
            "episode 34 score 22.0 avg score 19.4 time_steps 679 learning_steps 33\n",
            "episode 35 score 35.0 avg score 19.8 time_steps 714 learning_steps 35\n",
            "episode 36 score 47.0 avg score 20.6 time_steps 761 learning_steps 38\n",
            "episode 37 score 24.0 avg score 20.7 time_steps 785 learning_steps 39\n",
            "episode 38 score 48.0 avg score 21.4 time_steps 833 learning_steps 41\n",
            "episode 39 score 42.0 avg score 21.9 time_steps 875 learning_steps 43\n",
            "episode 40 score 61.0 avg score 22.8 time_steps 936 learning_steps 46\n",
            "episode 41 score 41.0 avg score 23.3 time_steps 977 learning_steps 48\n",
            "episode 42 score 57.0 avg score 24.0 time_steps 1034 learning_steps 51\n",
            "episode 43 score 74.0 avg score 25.2 time_steps 1108 learning_steps 55\n",
            "episode 44 score 81.0 avg score 26.4 time_steps 1189 learning_steps 59\n",
            "episode 45 score 37.0 avg score 26.7 time_steps 1226 learning_steps 61\n",
            "episode 46 score 97.0 avg score 28.1 time_steps 1323 learning_steps 66\n",
            "episode 47 score 57.0 avg score 28.8 time_steps 1380 learning_steps 69\n",
            "episode 48 score 115.0 avg score 30.5 time_steps 1495 learning_steps 74\n",
            "episode 49 score 75.0 avg score 31.4 time_steps 1570 learning_steps 78\n",
            "episode 50 score 95.0 avg score 32.6 time_steps 1665 learning_steps 83\n",
            "episode 51 score 143.0 avg score 34.8 time_steps 1808 learning_steps 90\n",
            "... saving models ...\n",
            "episode 52 score 113.0 avg score 36.2 time_steps 1921 learning_steps 96\n",
            "... saving models ...\n",
            "episode 53 score 68.0 avg score 36.8 time_steps 1989 learning_steps 99\n",
            "... saving models ...\n",
            "episode 54 score 169.0 avg score 39.2 time_steps 2158 learning_steps 107\n",
            "... saving models ...\n",
            "episode 55 score 59.0 avg score 39.6 time_steps 2217 learning_steps 110\n",
            "... saving models ...\n",
            "episode 56 score 60.0 avg score 39.9 time_steps 2277 learning_steps 113\n",
            "episode 57 score 22.0 avg score 39.6 time_steps 2299 learning_steps 114\n",
            "... saving models ...\n",
            "episode 58 score 77.0 avg score 40.3 time_steps 2376 learning_steps 118\n",
            "... saving models ...\n",
            "episode 59 score 200.0 avg score 42.9 time_steps 2576 learning_steps 128\n",
            "... saving models ...\n",
            "episode 60 score 46.0 avg score 43.0 time_steps 2622 learning_steps 131\n",
            "... saving models ...\n",
            "episode 61 score 127.0 avg score 44.3 time_steps 2749 learning_steps 137\n",
            "episode 62 score 34.0 avg score 44.2 time_steps 2783 learning_steps 139\n",
            "... saving models ...\n",
            "episode 63 score 87.0 avg score 44.8 time_steps 2870 learning_steps 143\n",
            "... saving models ...\n",
            "episode 64 score 89.0 avg score 45.5 time_steps 2959 learning_steps 147\n",
            "... saving models ...\n",
            "episode 65 score 151.0 avg score 47.1 time_steps 3110 learning_steps 155\n",
            "... saving models ...\n",
            "episode 66 score 198.0 avg score 49.4 time_steps 3308 learning_steps 165\n",
            "episode 67 score 42.0 avg score 49.3 time_steps 3350 learning_steps 167\n",
            "... saving models ...\n",
            "episode 68 score 167.0 avg score 51.0 time_steps 3517 learning_steps 175\n",
            "... saving models ...\n",
            "episode 69 score 200.0 avg score 53.1 time_steps 3717 learning_steps 185\n",
            "... saving models ...\n",
            "episode 70 score 140.0 avg score 54.3 time_steps 3857 learning_steps 192\n",
            "... saving models ...\n",
            "episode 71 score 200.0 avg score 56.3 time_steps 4057 learning_steps 202\n",
            "... saving models ...\n",
            "episode 72 score 185.0 avg score 58.1 time_steps 4242 learning_steps 212\n",
            "... saving models ...\n",
            "episode 73 score 200.0 avg score 60.0 time_steps 4442 learning_steps 222\n",
            "... saving models ...\n",
            "episode 74 score 155.0 avg score 61.3 time_steps 4597 learning_steps 229\n",
            "... saving models ...\n",
            "episode 75 score 156.0 avg score 62.5 time_steps 4753 learning_steps 237\n",
            "... saving models ...\n",
            "episode 76 score 126.0 avg score 63.4 time_steps 4879 learning_steps 243\n",
            "... saving models ...\n",
            "episode 77 score 101.0 avg score 63.8 time_steps 4980 learning_steps 249\n",
            "... saving models ...\n",
            "episode 78 score 170.0 avg score 65.2 time_steps 5150 learning_steps 257\n",
            "... saving models ...\n",
            "episode 79 score 135.0 avg score 66.1 time_steps 5285 learning_steps 264\n",
            "... saving models ...\n",
            "episode 80 score 129.0 avg score 66.8 time_steps 5414 learning_steps 270\n",
            "... saving models ...\n",
            "episode 81 score 194.0 avg score 68.4 time_steps 5608 learning_steps 280\n",
            "... saving models ...\n",
            "episode 82 score 132.0 avg score 69.2 time_steps 5740 learning_steps 287\n",
            "... saving models ...\n",
            "episode 83 score 106.0 avg score 69.6 time_steps 5846 learning_steps 292\n",
            "... saving models ...\n",
            "episode 84 score 114.0 avg score 70.1 time_steps 5960 learning_steps 298\n",
            "... saving models ...\n",
            "episode 85 score 121.0 avg score 70.7 time_steps 6081 learning_steps 304\n",
            "... saving models ...\n",
            "episode 86 score 119.0 avg score 71.3 time_steps 6200 learning_steps 310\n",
            "... saving models ...\n",
            "episode 87 score 128.0 avg score 71.9 time_steps 6328 learning_steps 316\n",
            "... saving models ...\n",
            "episode 88 score 172.0 avg score 73.0 time_steps 6500 learning_steps 325\n",
            "... saving models ...\n",
            "episode 89 score 130.0 avg score 73.7 time_steps 6630 learning_steps 331\n",
            "... saving models ...\n",
            "episode 90 score 120.0 avg score 74.2 time_steps 6750 learning_steps 337\n",
            "... saving models ...\n",
            "episode 91 score 172.0 avg score 75.2 time_steps 6922 learning_steps 346\n",
            "... saving models ...\n",
            "episode 92 score 200.0 avg score 76.6 time_steps 7122 learning_steps 356\n",
            "... saving models ...\n",
            "episode 93 score 174.0 avg score 77.6 time_steps 7296 learning_steps 364\n",
            "... saving models ...\n",
            "episode 94 score 185.0 avg score 78.7 time_steps 7481 learning_steps 374\n",
            "... saving models ...\n",
            "episode 95 score 195.0 avg score 80.0 time_steps 7676 learning_steps 383\n",
            "... saving models ...\n",
            "episode 96 score 200.0 avg score 81.2 time_steps 7876 learning_steps 393\n",
            "... saving models ...\n",
            "episode 97 score 200.0 avg score 82.4 time_steps 8076 learning_steps 403\n",
            "... saving models ...\n",
            "episode 98 score 200.0 avg score 83.6 time_steps 8276 learning_steps 413\n",
            "... saving models ...\n",
            "episode 99 score 134.0 avg score 84.1 time_steps 8410 learning_steps 420\n",
            "... saving models ...\n",
            "episode 100 score 160.0 avg score 85.3 time_steps 8570 learning_steps 428\n",
            "... saving models ...\n",
            "episode 101 score 200.0 avg score 87.2 time_steps 8770 learning_steps 438\n",
            "... saving models ...\n",
            "episode 102 score 138.0 avg score 88.5 time_steps 8908 learning_steps 445\n",
            "... saving models ...\n",
            "episode 103 score 151.0 avg score 89.8 time_steps 9059 learning_steps 452\n",
            "... saving models ...\n",
            "episode 104 score 196.0 avg score 91.6 time_steps 9255 learning_steps 462\n",
            "... saving models ...\n",
            "episode 105 score 148.0 avg score 92.8 time_steps 9403 learning_steps 470\n",
            "... saving models ...\n",
            "episode 106 score 167.0 avg score 94.3 time_steps 9570 learning_steps 478\n",
            "... saving models ...\n",
            "episode 107 score 200.0 avg score 96.1 time_steps 9770 learning_steps 488\n",
            "... saving models ...\n",
            "episode 108 score 200.0 avg score 97.9 time_steps 9970 learning_steps 498\n",
            "... saving models ...\n",
            "episode 109 score 158.0 avg score 99.4 time_steps 10128 learning_steps 506\n",
            "... saving models ...\n",
            "episode 110 score 130.0 avg score 100.5 time_steps 10258 learning_steps 512\n",
            "... saving models ...\n",
            "episode 111 score 183.0 avg score 102.1 time_steps 10441 learning_steps 522\n",
            "... saving models ...\n",
            "episode 112 score 162.0 avg score 103.6 time_steps 10603 learning_steps 530\n",
            "... saving models ...\n",
            "episode 113 score 196.0 avg score 105.3 time_steps 10799 learning_steps 539\n",
            "... saving models ...\n",
            "episode 114 score 155.0 avg score 106.7 time_steps 10954 learning_steps 547\n",
            "... saving models ...\n",
            "episode 115 score 200.0 avg score 108.4 time_steps 11154 learning_steps 557\n",
            "... saving models ...\n",
            "episode 116 score 134.0 avg score 109.5 time_steps 11288 learning_steps 564\n",
            "... saving models ...\n",
            "episode 117 score 200.0 avg score 111.4 time_steps 11488 learning_steps 574\n",
            "... saving models ...\n",
            "episode 118 score 163.0 avg score 112.8 time_steps 11651 learning_steps 582\n",
            "... saving models ...\n",
            "episode 119 score 195.0 avg score 114.7 time_steps 11846 learning_steps 592\n",
            "... saving models ...\n",
            "episode 120 score 144.0 avg score 116.0 time_steps 11990 learning_steps 599\n",
            "... saving models ...\n",
            "episode 121 score 200.0 avg score 117.9 time_steps 12190 learning_steps 609\n",
            "... saving models ...\n",
            "episode 122 score 158.0 avg score 119.4 time_steps 12348 learning_steps 617\n",
            "... saving models ...\n",
            "episode 123 score 200.0 avg score 121.2 time_steps 12548 learning_steps 627\n",
            "... saving models ...\n",
            "episode 124 score 165.0 avg score 122.7 time_steps 12713 learning_steps 635\n",
            "... saving models ...\n",
            "episode 125 score 163.0 avg score 124.0 time_steps 12876 learning_steps 643\n",
            "... saving models ...\n",
            "episode 126 score 200.0 avg score 125.9 time_steps 13076 learning_steps 653\n",
            "... saving models ...\n",
            "episode 127 score 200.0 avg score 127.8 time_steps 13276 learning_steps 663\n",
            "... saving models ...\n",
            "episode 128 score 200.0 avg score 129.6 time_steps 13476 learning_steps 673\n",
            "... saving models ...\n",
            "episode 129 score 200.0 avg score 131.1 time_steps 13676 learning_steps 683\n",
            "... saving models ...\n",
            "episode 130 score 200.0 avg score 132.8 time_steps 13876 learning_steps 693\n",
            "... saving models ...\n",
            "episode 131 score 200.0 avg score 134.5 time_steps 14076 learning_steps 703\n",
            "... saving models ...\n",
            "episode 132 score 114.0 avg score 135.4 time_steps 14190 learning_steps 709\n",
            "... saving models ...\n",
            "episode 133 score 157.0 avg score 136.9 time_steps 14347 learning_steps 717\n",
            "... saving models ...\n",
            "episode 134 score 200.0 avg score 138.7 time_steps 14547 learning_steps 727\n",
            "... saving models ...\n",
            "episode 135 score 197.0 avg score 140.3 time_steps 14744 learning_steps 737\n",
            "... saving models ...\n",
            "episode 136 score 200.0 avg score 141.8 time_steps 14944 learning_steps 747\n",
            "... saving models ...\n",
            "episode 137 score 200.0 avg score 143.6 time_steps 15144 learning_steps 757\n",
            "... saving models ...\n",
            "episode 138 score 200.0 avg score 145.1 time_steps 15344 learning_steps 767\n",
            "... saving models ...\n",
            "episode 139 score 200.0 avg score 146.7 time_steps 15544 learning_steps 777\n",
            "... saving models ...\n",
            "episode 140 score 200.0 avg score 148.1 time_steps 15744 learning_steps 787\n",
            "... saving models ...\n",
            "episode 141 score 200.0 avg score 149.7 time_steps 15944 learning_steps 797\n",
            "... saving models ...\n",
            "episode 142 score 200.0 avg score 151.1 time_steps 16144 learning_steps 807\n",
            "... saving models ...\n",
            "episode 143 score 200.0 avg score 152.4 time_steps 16344 learning_steps 817\n",
            "... saving models ...\n",
            "episode 144 score 200.0 avg score 153.6 time_steps 16544 learning_steps 827\n",
            "... saving models ...\n",
            "episode 145 score 200.0 avg score 155.2 time_steps 16744 learning_steps 837\n",
            "... saving models ...\n",
            "episode 146 score 200.0 avg score 156.2 time_steps 16944 learning_steps 847\n",
            "... saving models ...\n",
            "episode 147 score 90.0 avg score 156.5 time_steps 17034 learning_steps 851\n",
            "... saving models ...\n",
            "episode 148 score 200.0 avg score 157.4 time_steps 17234 learning_steps 861\n",
            "... saving models ...\n",
            "episode 149 score 200.0 avg score 158.6 time_steps 17434 learning_steps 871\n",
            "episode 150 score 26.0 avg score 157.9 time_steps 17460 learning_steps 873\n",
            "episode 151 score 200.0 avg score 158.5 time_steps 17660 learning_steps 883\n",
            "... saving models ...\n",
            "episode 152 score 200.0 avg score 159.4 time_steps 17860 learning_steps 893\n",
            "... saving models ...\n",
            "episode 153 score 200.0 avg score 160.7 time_steps 18060 learning_steps 903\n",
            "... saving models ...\n",
            "episode 154 score 200.0 avg score 161.0 time_steps 18260 learning_steps 913\n",
            "... saving models ...\n",
            "episode 155 score 200.0 avg score 162.4 time_steps 18460 learning_steps 923\n",
            "... saving models ...\n",
            "episode 156 score 200.0 avg score 163.8 time_steps 18660 learning_steps 933\n",
            "... saving models ...\n",
            "episode 157 score 200.0 avg score 165.6 time_steps 18860 learning_steps 943\n",
            "... saving models ...\n",
            "episode 158 score 200.0 avg score 166.8 time_steps 19060 learning_steps 953\n",
            "episode 159 score 200.0 avg score 166.8 time_steps 19260 learning_steps 963\n",
            "... saving models ...\n",
            "episode 160 score 200.0 avg score 168.4 time_steps 19460 learning_steps 973\n",
            "... saving models ...\n",
            "episode 161 score 200.0 avg score 169.1 time_steps 19660 learning_steps 983\n",
            "... saving models ...\n",
            "episode 162 score 182.0 avg score 170.6 time_steps 19842 learning_steps 992\n",
            "episode 163 score 33.0 avg score 170.1 time_steps 19875 learning_steps 993\n",
            "... saving models ...\n",
            "episode 164 score 200.0 avg score 171.2 time_steps 20075 learning_steps 1003\n",
            "... saving models ...\n",
            "episode 165 score 161.0 avg score 171.3 time_steps 20236 learning_steps 1011\n",
            "episode 166 score 148.0 avg score 170.8 time_steps 20384 learning_steps 1019\n",
            "... saving models ...\n",
            "episode 167 score 162.0 avg score 172.0 time_steps 20546 learning_steps 1027\n",
            "episode 168 score 145.0 avg score 171.7 time_steps 20691 learning_steps 1034\n",
            "episode 169 score 154.0 avg score 171.3 time_steps 20845 learning_steps 1042\n",
            "episode 170 score 122.0 avg score 171.1 time_steps 20967 learning_steps 1048\n",
            "episode 171 score 133.0 avg score 170.4 time_steps 21100 learning_steps 1055\n",
            "episode 172 score 139.0 avg score 170.0 time_steps 21239 learning_steps 1061\n",
            "episode 173 score 181.0 avg score 169.8 time_steps 21420 learning_steps 1071\n",
            "episode 174 score 173.0 avg score 170.0 time_steps 21593 learning_steps 1079\n",
            "episode 175 score 200.0 avg score 170.4 time_steps 21793 learning_steps 1089\n",
            "episode 176 score 200.0 avg score 171.1 time_steps 21993 learning_steps 1099\n",
            "... saving models ...\n",
            "episode 177 score 200.0 avg score 172.1 time_steps 22193 learning_steps 1109\n",
            "... saving models ...\n",
            "episode 178 score 200.0 avg score 172.4 time_steps 22393 learning_steps 1119\n",
            "... saving models ...\n",
            "episode 179 score 194.0 avg score 173.0 time_steps 22587 learning_steps 1129\n",
            "... saving models ...\n",
            "episode 180 score 200.0 avg score 173.7 time_steps 22787 learning_steps 1139\n",
            "... saving models ...\n",
            "episode 181 score 200.0 avg score 173.8 time_steps 22987 learning_steps 1149\n",
            "... saving models ...\n",
            "episode 182 score 199.0 avg score 174.5 time_steps 23186 learning_steps 1159\n",
            "... saving models ...\n",
            "episode 183 score 200.0 avg score 175.4 time_steps 23386 learning_steps 1169\n",
            "... saving models ...\n",
            "episode 184 score 200.0 avg score 176.3 time_steps 23586 learning_steps 1179\n",
            "... saving models ...\n",
            "episode 185 score 200.0 avg score 177.1 time_steps 23786 learning_steps 1189\n",
            "... saving models ...\n",
            "episode 186 score 200.0 avg score 177.9 time_steps 23986 learning_steps 1199\n",
            "... saving models ...\n",
            "episode 187 score 200.0 avg score 178.6 time_steps 24186 learning_steps 1209\n",
            "... saving models ...\n",
            "episode 188 score 200.0 avg score 178.9 time_steps 24386 learning_steps 1219\n",
            "... saving models ...\n",
            "episode 189 score 200.0 avg score 179.6 time_steps 24586 learning_steps 1229\n",
            "... saving models ...\n",
            "episode 190 score 200.0 avg score 180.4 time_steps 24786 learning_steps 1239\n",
            "... saving models ...\n",
            "episode 191 score 200.0 avg score 180.6 time_steps 24986 learning_steps 1249\n",
            "episode 192 score 200.0 avg score 180.6 time_steps 25186 learning_steps 1259\n",
            "... saving models ...\n",
            "episode 193 score 200.0 avg score 180.9 time_steps 25386 learning_steps 1269\n",
            "... saving models ...\n",
            "episode 194 score 200.0 avg score 181.1 time_steps 25586 learning_steps 1279\n",
            "... saving models ...\n",
            "episode 195 score 200.0 avg score 181.1 time_steps 25786 learning_steps 1289\n",
            "episode 196 score 200.0 avg score 181.1 time_steps 25986 learning_steps 1299\n",
            "episode 197 score 200.0 avg score 181.1 time_steps 26186 learning_steps 1309\n",
            "episode 198 score 200.0 avg score 181.1 time_steps 26386 learning_steps 1319\n",
            "... saving models ...\n",
            "episode 199 score 200.0 avg score 181.8 time_steps 26586 learning_steps 1329\n",
            "... saving models ...\n",
            "episode 200 score 200.0 avg score 182.2 time_steps 26786 learning_steps 1339\n",
            "episode 201 score 200.0 avg score 182.2 time_steps 26986 learning_steps 1349\n",
            "... saving models ...\n",
            "episode 202 score 200.0 avg score 182.8 time_steps 27186 learning_steps 1359\n",
            "... saving models ...\n",
            "episode 203 score 200.0 avg score 183.3 time_steps 27386 learning_steps 1369\n",
            "... saving models ...\n",
            "episode 204 score 200.0 avg score 183.3 time_steps 27586 learning_steps 1379\n",
            "... saving models ...\n",
            "episode 205 score 200.0 avg score 183.8 time_steps 27786 learning_steps 1389\n",
            "... saving models ...\n",
            "episode 206 score 200.0 avg score 184.2 time_steps 27986 learning_steps 1399\n",
            "episode 207 score 200.0 avg score 184.2 time_steps 28186 learning_steps 1409\n",
            "episode 208 score 200.0 avg score 184.2 time_steps 28386 learning_steps 1419\n",
            "... saving models ...\n",
            "episode 209 score 200.0 avg score 184.6 time_steps 28586 learning_steps 1429\n",
            "... saving models ...\n",
            "episode 210 score 200.0 avg score 185.3 time_steps 28786 learning_steps 1439\n",
            "... saving models ...\n",
            "episode 211 score 200.0 avg score 185.4 time_steps 28986 learning_steps 1449\n",
            "... saving models ...\n",
            "episode 212 score 200.0 avg score 185.8 time_steps 29186 learning_steps 1459\n",
            "... saving models ...\n",
            "episode 213 score 200.0 avg score 185.9 time_steps 29386 learning_steps 1469\n",
            "... saving models ...\n",
            "episode 214 score 200.0 avg score 186.3 time_steps 29586 learning_steps 1479\n",
            "episode 215 score 200.0 avg score 186.3 time_steps 29786 learning_steps 1489\n",
            "... saving models ...\n",
            "episode 216 score 200.0 avg score 187.0 time_steps 29986 learning_steps 1499\n",
            "episode 217 score 200.0 avg score 187.0 time_steps 30186 learning_steps 1509\n",
            "... saving models ...\n",
            "episode 218 score 200.0 avg score 187.3 time_steps 30386 learning_steps 1519\n",
            "... saving models ...\n",
            "episode 219 score 200.0 avg score 187.4 time_steps 30586 learning_steps 1529\n",
            "... saving models ...\n",
            "episode 220 score 200.0 avg score 188.0 time_steps 30786 learning_steps 1539\n",
            "episode 221 score 200.0 avg score 188.0 time_steps 30986 learning_steps 1549\n",
            "... saving models ...\n",
            "episode 222 score 200.0 avg score 188.4 time_steps 31186 learning_steps 1559\n",
            "episode 223 score 200.0 avg score 188.4 time_steps 31386 learning_steps 1569\n",
            "... saving models ...\n",
            "episode 224 score 200.0 avg score 188.7 time_steps 31586 learning_steps 1579\n",
            "... saving models ...\n",
            "episode 225 score 200.0 avg score 189.1 time_steps 31786 learning_steps 1589\n",
            "episode 226 score 200.0 avg score 189.1 time_steps 31986 learning_steps 1599\n",
            "episode 227 score 200.0 avg score 189.1 time_steps 32186 learning_steps 1609\n",
            "episode 228 score 153.0 avg score 188.6 time_steps 32339 learning_steps 1616\n",
            "episode 229 score 200.0 avg score 188.6 time_steps 32539 learning_steps 1626\n",
            "episode 230 score 200.0 avg score 188.6 time_steps 32739 learning_steps 1636\n",
            "episode 231 score 200.0 avg score 188.6 time_steps 32939 learning_steps 1646\n",
            "... saving models ...\n",
            "episode 232 score 200.0 avg score 189.5 time_steps 33139 learning_steps 1656\n",
            "... saving models ...\n",
            "episode 233 score 200.0 avg score 189.9 time_steps 33339 learning_steps 1666\n",
            "episode 234 score 200.0 avg score 189.9 time_steps 33539 learning_steps 1676\n",
            "... saving models ...\n",
            "episode 235 score 200.0 avg score 189.9 time_steps 33739 learning_steps 1686\n",
            "episode 236 score 200.0 avg score 189.9 time_steps 33939 learning_steps 1696\n",
            "episode 237 score 200.0 avg score 189.9 time_steps 34139 learning_steps 1706\n",
            "episode 238 score 200.0 avg score 189.9 time_steps 34339 learning_steps 1716\n",
            "episode 239 score 200.0 avg score 189.9 time_steps 34539 learning_steps 1726\n",
            "episode 240 score 200.0 avg score 189.9 time_steps 34739 learning_steps 1736\n",
            "episode 241 score 200.0 avg score 189.9 time_steps 34939 learning_steps 1746\n",
            "episode 242 score 200.0 avg score 189.9 time_steps 35139 learning_steps 1756\n",
            "episode 243 score 200.0 avg score 189.9 time_steps 35339 learning_steps 1766\n",
            "episode 244 score 200.0 avg score 189.9 time_steps 35539 learning_steps 1776\n",
            "episode 245 score 200.0 avg score 189.9 time_steps 35739 learning_steps 1786\n",
            "episode 246 score 200.0 avg score 189.9 time_steps 35939 learning_steps 1796\n",
            "... saving models ...\n",
            "episode 247 score 200.0 avg score 191.1 time_steps 36139 learning_steps 1806\n",
            "episode 248 score 200.0 avg score 191.1 time_steps 36339 learning_steps 1816\n",
            "episode 249 score 200.0 avg score 191.1 time_steps 36539 learning_steps 1826\n",
            "... saving models ...\n",
            "episode 250 score 200.0 avg score 192.8 time_steps 36739 learning_steps 1836\n",
            "episode 251 score 71.0 avg score 191.5 time_steps 36810 learning_steps 1840\n",
            "episode 252 score 200.0 avg score 191.5 time_steps 37010 learning_steps 1850\n",
            "episode 253 score 200.0 avg score 191.5 time_steps 37210 learning_steps 1860\n",
            "episode 254 score 200.0 avg score 191.5 time_steps 37410 learning_steps 1870\n",
            "episode 255 score 200.0 avg score 191.5 time_steps 37610 learning_steps 1880\n",
            "episode 256 score 200.0 avg score 191.5 time_steps 37810 learning_steps 1890\n",
            "episode 257 score 200.0 avg score 191.5 time_steps 38010 learning_steps 1900\n",
            "episode 258 score 200.0 avg score 191.5 time_steps 38210 learning_steps 1910\n",
            "episode 259 score 200.0 avg score 191.5 time_steps 38410 learning_steps 1920\n",
            "episode 260 score 200.0 avg score 191.5 time_steps 38610 learning_steps 1930\n",
            "episode 261 score 200.0 avg score 191.5 time_steps 38810 learning_steps 1940\n",
            "episode 262 score 200.0 avg score 191.7 time_steps 39010 learning_steps 1950\n",
            "... saving models ...\n",
            "episode 263 score 200.0 avg score 193.3 time_steps 39210 learning_steps 1960\n",
            "episode 264 score 200.0 avg score 193.3 time_steps 39410 learning_steps 1970\n",
            "... saving models ...\n",
            "episode 265 score 200.0 avg score 193.7 time_steps 39610 learning_steps 1980\n",
            "... saving models ...\n",
            "episode 266 score 200.0 avg score 194.3 time_steps 39810 learning_steps 1990\n",
            "... saving models ...\n",
            "episode 267 score 193.0 avg score 194.6 time_steps 40003 learning_steps 2000\n",
            "... saving models ...\n",
            "episode 268 score 200.0 avg score 195.1 time_steps 40203 learning_steps 2010\n",
            "... saving models ...\n",
            "episode 269 score 200.0 avg score 195.6 time_steps 40403 learning_steps 2020\n",
            "... saving models ...\n",
            "episode 270 score 200.0 avg score 196.4 time_steps 40603 learning_steps 2030\n",
            "... saving models ...\n",
            "episode 271 score 200.0 avg score 197.0 time_steps 40803 learning_steps 2040\n",
            "... saving models ...\n",
            "episode 272 score 200.0 avg score 197.6 time_steps 41003 learning_steps 2050\n",
            "... saving models ...\n",
            "episode 273 score 200.0 avg score 197.8 time_steps 41203 learning_steps 2060\n",
            "... saving models ...\n",
            "episode 274 score 200.0 avg score 198.1 time_steps 41403 learning_steps 2070\n",
            "episode 275 score 200.0 avg score 198.1 time_steps 41603 learning_steps 2080\n",
            "episode 276 score 200.0 avg score 198.1 time_steps 41803 learning_steps 2090\n",
            "episode 277 score 200.0 avg score 198.1 time_steps 42003 learning_steps 2100\n",
            "episode 278 score 200.0 avg score 198.1 time_steps 42203 learning_steps 2110\n",
            "... saving models ...\n",
            "episode 279 score 200.0 avg score 198.2 time_steps 42403 learning_steps 2120\n",
            "episode 280 score 200.0 avg score 198.2 time_steps 42603 learning_steps 2130\n",
            "episode 281 score 200.0 avg score 198.2 time_steps 42803 learning_steps 2140\n",
            "... saving models ...\n",
            "episode 282 score 200.0 avg score 198.2 time_steps 43003 learning_steps 2150\n",
            "episode 283 score 200.0 avg score 198.2 time_steps 43203 learning_steps 2160\n",
            "episode 284 score 200.0 avg score 198.2 time_steps 43403 learning_steps 2170\n",
            "episode 285 score 200.0 avg score 198.2 time_steps 43603 learning_steps 2180\n",
            "episode 286 score 200.0 avg score 198.2 time_steps 43803 learning_steps 2190\n",
            "episode 287 score 200.0 avg score 198.2 time_steps 44003 learning_steps 2200\n",
            "episode 288 score 200.0 avg score 198.2 time_steps 44203 learning_steps 2210\n",
            "episode 289 score 200.0 avg score 198.2 time_steps 44403 learning_steps 2220\n",
            "episode 290 score 200.0 avg score 198.2 time_steps 44603 learning_steps 2230\n",
            "episode 291 score 200.0 avg score 198.2 time_steps 44803 learning_steps 2240\n",
            "episode 292 score 200.0 avg score 198.2 time_steps 45003 learning_steps 2250\n",
            "episode 293 score 200.0 avg score 198.2 time_steps 45203 learning_steps 2260\n",
            "episode 294 score 200.0 avg score 198.2 time_steps 45403 learning_steps 2270\n",
            "episode 295 score 200.0 avg score 198.2 time_steps 45603 learning_steps 2280\n",
            "episode 296 score 200.0 avg score 198.2 time_steps 45803 learning_steps 2290\n",
            "episode 297 score 200.0 avg score 198.2 time_steps 46003 learning_steps 2300\n",
            "episode 298 score 200.0 avg score 198.2 time_steps 46203 learning_steps 2310\n",
            "episode 299 score 200.0 avg score 198.2 time_steps 46403 learning_steps 2320\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}